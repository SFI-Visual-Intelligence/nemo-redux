#!/usr/bin/env bash

set -eu

function random-string() {
  # shellcheck disable=SC2002
  cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w "${1:-32}" | head -n 1
}

suite_id="$(random-string 4)"
echo "Launching suite with ID \"${suite_id}\"..."

# I/O options.
data_dir="/data/segmentation-resized/partitioned/combined"
output_dir="output/detection/suite/${suite_id}"
checkpoint_interval="5"

# Optimizer parameters.
optimizer="adamw"
learning_rate="1e-5"
weight_decay="1e-4"

# Learning rate scheduler parameters.
lr_milestones="25,40,45"
lr_gamma="0.1"

# Other options...
dropout_rate="0.2"
trainable_backbone_layers="3"
backbone_epochs="25"
max_epochs="50"
num_workers="0"

# Stochasticity parameters.
seed=42  # Make suite deterministic.

# Suite params.
num_repetitions=10

# Attempt to speed things up by copying data to host.
host_dir="/tmp/data"
echo "Copying data from ${data_dir} -> ${host_dir}..."
cp -r "${data_dir}" "${host_dir}"
data_dir="${host_dir}"

echo "Running experiments"
echo "-------------------"

# Train some non-stochastic models...
for ((i=0; i<num_repetitions; i++)); do
  echo -e "\n** Non-stochastic run #$((i+1))..."
  python src/train_detector.py \
    --seed=$((seed+i)) \
    --dropout-rate="0" \
    --data-dir="${data_dir}" \
    --output-dir="${output_dir}" \
    --checkpoint-interval="${checkpoint_interval}" \
    --optimizer="${optimizer}" \
    --learning-rate="${learning_rate}" \
    --weight-decay="${weight_decay}" \
    --lr-milestones="${lr_milestones}" \
    --lr-gamma="${lr_gamma}" \
    --trainable-backbone-layers="${trainable_backbone_layers}" \
    --backbone-epochs="${backbone_epochs}" \
    --max-epochs="${max_epochs}" \
    --num-workers=${num_workers}
done

# Train some models with dropout sampling...
# for ((i=0; i<num_repetitions; i++)); do
#   echo -e "\n** Stochastic run #$((i+1))..."
#   python src/train_detector.py \
#     --seed=$((seed+i)) \
#     --dropout-rate="${dropout_rate}" \
#     --data-dir="${data_dir}" \
#     --output-dir="${output_dir}" \
#     --checkpoint-interval="${checkpoint_interval}" \
#     --optimizer="${optimizer}" \
#     --learning-rate="${learning_rate}" \
#     --weight-decay="${weight_decay}" \
#     --lr-milestones="${lr_milestones}" \
#     --lr-gamma="${lr_gamma}" \
#     --trainable-backbone-layers="${trainable_backbone_layers}" \
#     --backbone-epochs="${backbone_epochs}" \
#     --max-epochs="${max_epochs}" \
#     --num-workers=${num_workers}
# done
